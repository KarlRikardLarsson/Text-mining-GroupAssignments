import pandas as pd
import spacy
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from tabulate import tabulate
import io


#Loading English pipeline language model and assign it to a vairable
tokenizer = spacy.load("en_core_web_sm")

nlp = spacy.load("en_core_web_sm")
#Being able to print full column content, not truncating the results
pd.set_option('display.max_colwidth', 110)

#read file with specific delimiter and only the column with sentences
df = pd.read_csv(io.BytesIO(uploaded['amazon_cells_labelled.csv']), sep=';', header=None, usecols=[0])
#rename sentences column
df.rename(columns={0: 'sentence'}, inplace=True)
#sample 100 sentences out of the dataset
#df = df.sample(n=100) 
df = df.head(100)
    
#to lowercase
df['sentence'] = df['sentence'].str.lower()
#remove line endings
df = df.replace('[\.\?!]+$','', regex=True)
#remove quotes
df = df.replace('[\"]+', '', regex=True)
#replace sentence breaks with spaces
df = df.replace('[\.,]+', ' ', regex=True)
#replace spaces with one space
df.sentence = df.sentence.replace('\s+',' ', regex=True)

#removes label for sentiment that's present at the end of the sentence
df.sentence = df.sentence.str[:-1]

#create tokens column and create tokens for every row from sentences
df['tokens'] = ''
for index, row in df.iterrows():
    sentence_tokens = tokenizer(row['sentence'])
    sentence_token_list = []
    for x in sentence_tokens:
        sentence_token_list.append(x.text)
    df['tokens'][index] = sentence_token_list
    
    
    
    #iterate over rows and get BLEU of current row according to next 19 rows
#last 19 rows are ignored atm
df['sentence_bleu'] = ''
df['corpus_bleu'] = ''
chencherry = SmoothingFunction()
for index, row in df.iloc[:-19].iterrows():
    references = []
    for x in range(index+1,index+20):
        references.append(df['tokens'][x]) 
    df['sentence_bleu'][index] = sentence_bleu(references, row['tokens'],weights=(0.25,0.25,0.15,0.35),smoothing_function=chencherry.method1)

#do a corpus bleu for the whole dataset
references = []
candidates = []
for index, row in df.iloc[:].iterrows():
  if (index < 50):
    references.append(row['tokens'])
  else:
    candidates.append(row['tokens'])
df['corpus_bleu'][0] = corpus_bleu(references, candidates,weights=(0.5,0.25,0.15,0.1),smoothing_function=chencherry.method1)

print(tabulate(df, headers='keys', tablefmt='psql'))
